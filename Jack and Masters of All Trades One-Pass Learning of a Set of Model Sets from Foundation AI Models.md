##  论文笔记：《 Jack and Masters of All Trades: One-Pass Learning of a Set of Model Sets from Foundation AI Models 》

中译：翻不出来hh，无论怎么组织语言都觉得怪 :(

总结：

原文连接：https://arxiv.org/pdf/2205.00671v2.pdf

## ●Abstract

-   question作者想解决什么问题？
    
    我们都知道，根据大量数据训练的大规模神经网络可以完成深度学习中的许多任务，这里我们把各个任务统称为基础模型 (foundation models) 或者 ’ Jacks of All Trades ’ (JATs)， Jacks of All Trades在英语当中是一句谚语：全才，啥都会，但都不精通。但是大规模的神经网络意味着消耗的计算资源会更多，况且任务需求和目标也是不断变化的，使得单一的 JAT在现实世界中的可用性受到限制，所以**本文要做的工作的就是扩建这个基础模型 JATs，使它能应用到更多不同的场景和任务当中**。
    
    
    
-   method作者通过什么理论/模型来解决这个问题？
    
    作者将各个特定的子模型 (Set) 组成一个模型集 (Sets)，即提出一个叫 **Set of Sets 的概念**，这个 Set of Sets 要同时满足每个子任务的设置和环境条件，提出了一种通过**神经进化多任务处理算法一次**就能追踪到这样一个集合的方法，得到这个集合 (Sets) 就相当于与 **MATs** (Master of All Trades) 这个概念更接近了，Master of All Trades 在文中的意思相当于：全才，啥都会，都精通。后面会对这个 MATs进行介绍。
    
    
    
-   answer作者给出的答案是什么？
    
     
    
    

## ●Instruction

-   why作者为什么研究这个课题
    
    由一个个基础模型组成的 JAT模型利用任务之间的相似性（比如获得它们的共同特征），从而允许同时学习多个任务。但是这种多面的学习有时会导致任务之间错误的权衡，导致 JAT在执行单个任务的时候性能减弱。所以说**需要学习的任务因为其复杂性，加上大小数量的不同，具有有限信息编码能力的 JAT 通常无法为每个任务分配专门的内部子结构**，**而且加上 JAT 计算量的消耗，它的扩展性很差**（比如换个应用场景或领域就没法用了，终端产品比如手机上也用不了）。作者就是想设计出一个接近 MATs概念的模型，这个模型能执行很多子任务，不减少精确度的同时还能减少更多的计算量，扩展性更强，方法用的就是神经进化多任务处理算法
    
    
    
-   how当前研究到了哪一阶段
    
    **MOTs: (Masters of One Trade) 指的是在一个特定任务上精确度高且计算量小的模型**
    
    ​	original LeNet (65K parameters); 
    
    ​	AlphaGo (4.6M parameters)
    
    **JATs:**
    
    ​	ViT-G/14  (1.8B parameters);
    
    ​	GPT-3 (175B parameters)
    
    
    
-   what作者基于什么样的假设（看不懂最后去查）
    
    ​	**如果一个任务在 MOT 模型运行的性能（精确度、计算量）足以超过在 JAT 模型上运行的性能，那么在多任务的情况下，这样的 MOT 的集合，即 MOTs，在功能上就能接近 MAT 。**但需要注意的是，MOT、JATs 和 MATs 不是独立的，其内部是相互依赖的。建立一个个用于运行具体任务的深度卷积神经网络要考虑许多计算因素，比如模型的计算量、以及在预训练好的 JATs 模型的参数利用率。如果从头开始训练一个 MOT 模型，就不能像 JAT 那样执行多个任务；如果从训练有素的 JAT 模型得到 MOT，即训练一个大模型，再将其减小到一个小模型，就可以有效解决计算因素的问题。
    
    
    
    (ei.....除了黑体部分，其他看的不是很懂，大概翻译整理过来是这样，有兴趣的话大家去看下原文)

## ●Conclusion

-   优点
    
    实验证实，从 JAT 中去除任务间干扰确实可以为压缩的JAT提供基础，使其在特定任务上获得更强的性能，并显著减小模型大小。
    
-   缺点
    
    仍需要对集合（子任务）进行全面的分析，以便扩大模型的实际应用范围；此外，在涉及各种资源受限的边缘设备的现实场景中实现，该概念的实用性将得到进一步的证实。
    
    在对 JAT DNN网络进行编码时，需要确保每层网络都保持一定的活跃性，即不能全为0，这是参数量的硬限制。

## ●Table & Method

-   数据来源（主要看黑体加粗字体，其他为实验设置）
    
    所有的实验使用AMD Threadripper 3990X和一台Nvidia 3090 RTX进行。为了保持 JAT 是一个大规模 DNN的前提，使用了**Facebook M2M100-418M**模型中预先训练的权重
    
    
    
    **Multilingual Translation: WMT19**
    
    ​		主要用到 WMT19 数据集中的 Czech-to-English 和 German-to-English，这两个数据集来自 HuggingFace Transformers 和 datasets libraries，预训练的M2M100-418M为两个翻译任务训练了65个epoch，批大小为1920，使用 AdamW 优化器。
    
    ​		二进制掩码的维数是67,584。它只适用于注意层，而不是词汇或位置嵌入，或预测头。
    
    ​		模型大小约为1.8 GB。然后使用MOMFEA同时为每种语言进化出20个专家模型，持续120代。在进化过程中，使用训练数据的负对数似然损失评估模型的适应度。
    
    ​		终止后，使用AdaFactor优化器对得到的解进行200个epoch的微调。经过微调后，对测试数据对模型进行BLEU评分评估。
    
    
    
    **Time-Series Regression: Beijing Multi-Site Air-Quality dataset (来自 UCI)**
    
    ​		这个数据集涵盖了北京周边12个监测点的每小时空气污染指数，目的是预测PM2.5水平。选择四个监测站的测量结果，每个监测站位于不同的地理区域。数据点每小时测量一次，从2013年3月1日到2017年2月28日。
    
    ​		一个双向LSTM，包含2层128个节点的隐藏层和1个全连接的输出层，在完整数据集上训练。使用10小时的时间窗口。二进制掩码的维数为4096，模型大小约为2.12 MB。在MO-MFEA中，每个领域对应的60名专家的人口进化了120代。进化之后，使用AdamW优化器对获得的候选解进行了15个epoch的微调，学习率为0.001，乘法衰减为0.99。均方根误差(RMSE)损失用于测试数据的演化、微调和最终评估。
    
    
    
    **Image Classification: MNIST and CIFAR-10**
    
    ​		由于预训练的ImageNet模型是现有基础模型的重要组成部分，因此使用**ResNet-18**。这个模型的大小大约是44 MB。此外，将参数的数据类型从32位浮点数更改为16位半浮点数。这将模型大小减少到22 MB。二进制掩码的维度是18,944。
    
    ​		我们为每一项任务演化选出60个个体，共120代。进化之后，使用Adam优化器对获得的候选对象进行微调，学习率为0.0005，衰减为0.95。MNIST模型微调10个epoch，而CIFAR模型微调25个。交叉熵损失用于进化和微调，并评估最终总体的分类精度测试数据。
    
    

-   重要指标

    
    $$
    \begin{align}
    &1、每个任务T_i由三个指标来评估:\\
    \\
    &其中D^i表示第 i 个数据集；F(x^i;\theta)指的是第i个数据集中的数据x放入由\theta初始化的DNN网络得到的预测值；\\
    & L^i(y ̂,y^i)中y ̂=F(x;\theta)，y^i为真实标签
    \end{align}
    $$

    $$
    T_i≜{D^i,F(x^i;\theta),L^i(y ̂,y^i)}
    $$

    $$
    \begin{align}
    & \theta可以是卷积神经网络、注意力机制模块或其他神经网络；F的目标是准确地获得真实数据的概率分布;\\
    & L^i可以是交叉熵、focal  -loss或用户提供的先验和首选项
    \end{align}
    $$




$$
\begin{align}
& 2、整体的DNN模型的参数用二进制编码表示，来计算参数量:‖β‖_1 \\
& 其中β=[b_1^1，b_1^2，b_1^3，…，b_1^{p_1}，b_2^1，b_2^2，b_2^3，…，b_L^{p_L}]\\
& L是DNN神经网络的层数，每层都有一个维度p_l，lϵ(1,L)，b_l^kϵ0或1 
\end{align}
$$



$$
\begin{align}
3、第i个任务的MOT表示为：\\
& min(θ^i)⁡\frac{1}{N_i}  \sum_{n=1}^{N_i}L^i (F(x_n^i;θ^i),y_n^i)       ；其中y^i ϵY^i\\
& 其中θ^i是任务T_i调优的模型参数集，Y_i是输出标签的集合 \\
\end{align}
$$



$$
\begin{align}
& 4、JAT处理的是最小化K个不同任务的平均损失，表示为：\\
& min(θ^{sh},θ^1 … θ^K )⁡\frac{1}{K} \sum_{1}^{K}\frac{1}{N_i}\sum_{n=1}^{N_i}L^i (F(x_n^i;θ^{sh},θ^i ),y_n^i)\\
& 它包含一个参数子集\theta^{sh}，\theta^{sh}除了拥有K组任务专用参数\theta^i外，所有任务都严格共享这个子集\\
\\
&（顺便说一句：也是因为这个平均损失导致JAT任务之间进行交互，从而影响JATs单个任务的性能）
\end{align}
$$



$$
\begin{align}
& 5、MAT表示为：\\
& \theta_{MAT}=⋃_{i=1}^Kθ^{i*}；∀T_iϵ1,2,3,...,K \\
& θ^{i*}=argmin\frac{1}{N_i}\sum_{n=1}^{N_i}L^i(F(x_{n}^{i};\theta^i),y_{n}^{i})\\
\end{align}
$$



$$
\begin{align}
& 6、Membership in the Set of Sets:\\
\\
&（“怎么成为天选之子”，直接截图了，懒狗不想打公式了呜呜，下面那个长得像G的东西就是pre-trained好的JAT大规模DNN）；
\end{align}
$$
<img src="C:\Users\Liz\Desktop\1.png" alt="1" style="zoom: 50%;" />




$$
\begin{align}
& 7、总体目标函数：
\end{align}
$$
<img src="C:\Users\Liz\Desktop\3.png" alt="3" style="zoom:50%;" />




$$
\begin{align}
& 8、演化计算：
\end{align}
$$

<img src="C:\Users\Liz\Desktop\2.png" alt="2" style="zoom:50%;" />




-   模型步骤（看不懂理论推导没关系）+ 每个步骤得出的结论

    实验研究使用了四种数据集，在多语言翻译、回归和图像分类领域。首先让一个代表 JAT 的**大规模DNN对每个数据集进行了完整的预训练**。然后**将每个数据集重新配置为许多任务**，每个任务代表数据类别的专门化，因此范围更窄。然后，专家会同时**为所有任务进化**，适应度评估使用两个目标函数:尺寸 size 和性能测量 performance 。最后的进化种群在任务特定的训练数据上进行微调(通过梯度下降)，然后在测试集上进行评估。

    

    上面这段是原文翻译过来滴，好像懂了又好像没懂哈哈哈...

    通过前后逻辑，我说说自己的理解：

    

    ​	首先将4个数据集放到 JAT 这个设计好的神经网络大模型中进行预训练，然后用上面提到的演化计算方法进行进化，把优秀的个体保留下来。具体演化方法为：

    1）从4个数据集中分别随机选取 a 个个体，并计算他们各自的参数量和损失值，此时选中的个体数量为4a；

    2）然后从这4a个个体中随机选2个作为父代进行二进制编码交叉变异，注意随机选择的2个父代如果来自同一数据集和不同数据集交叉和变异方式是不一样的，大家可以看上面的伪代码有3种方式
    
    3）子代用总体目标函数进行评估（见重要指标7，详细的见指标2和3），评估值越低的越优秀
    
    4）最终输出优秀个体的集合
    
    （上一part的指标2是给JAT这个大网络集合的参数都进行二进制编码，这有点离谱，所以文章中还提到了一个小技巧来避免繁重的编码过程：通过参数分组机制降低任意层 l 的有效维数ρl，有兴趣可以去看看。）
    
    
    
    ___________________________________________________________________________________________________________________________________________________________________________________________________________________________
    
    文章没有源码，有些细节也不好解释，读者见谅qaq....
    
    _________________________________________________________________________________________________________________________________________________________________________________________________________________________
    

# ●Lexical Expressions

**Buoyed by  被...鼓舞**

​	本人平常总是用 inspired by...，用烦了hh...以后得换换新词

**archetypal  原型，模型 n.**

​	平常基本都用 model，结果全文都用 model，笑死...

**unabated  有增无减的  adj.**

​	这个词很精炼，还用 There is increase but no decrease 吗？

**culmination  高潮，终点  n.**

​	目标我们总是用 goal, target, objective 或者 destination, 新鲜点可能还有 aim to/ be aimed to, 可以试试用 The culmination of this paper/pursuit is.... 来表示自己方法的最终目标

**sheer**  **完全的，数量庞大的  adj.**

​	庞大还可以用sheer表示，比如sheer size, 这样文中就不会重复出现 big, large....等等同义的词汇

**be encumbered by  受...限制**

​	也可以用 by the limit of... / be constrained by... / be subject to... 等等

**be conceived as  被视为...**

​	还可以用 be perceived as... / be regarded as... / be deemed as... / be considered as... / be seen as... 

**facet  方面 n.**

​	在某方面我们可以用 aspect, side, way等比较多

**Following-on on an example of.....  以下是一个...的例子**

**In keeping with the premise of...  基于....的前提下**

**Instantiation 实例化 n.**

